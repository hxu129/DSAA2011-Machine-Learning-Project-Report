\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025


% ready for submission
\usepackage[preprint]{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multicol}
\usepackage{multirow}
\usepackage{graphicx} % 虽然现在是占位符，但将来替换图片时会用到
\usepackage{subcaption} % 用于创建子图环境
\usepackage{makecell}
\usepackage{tabularray}

\title{Project Report \large \\ DSAA2011 Machine Learning}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  % examples of more authors
   Hua XU \\
   HKUST(GZ) \\
   \texttt{hxu401@connect.hkust-gz.edu.cn} \\
   \And
   Jianhao RUAN \\
   HKUST(GZ) \\
   \texttt{jruan189@connect.hkust-gz.edu.cn} \\
   \And
   Leyi WU \\
   HKUST(GZ) \\
   \texttt{lwu398@connect.hkust-gz.edu.cn} \\
}


\begin{document}


\maketitle

\section{Introduction}

\underline{don't forget to finish this part}


\section{Methods and results}

\underline{add an introductory sentence here}

\subsection{Data preprocessing}

\begin{itemize}
    \item \underline{observed patterns of dataset or insights}
\end{itemize}

The Adult Income dataset, employed in this project, is structured for binary classification and comprises 14 usable variables. These variables include six of integer type and eight of categorical type. Specific preprocessing strategies were applied to handle these different data types:

\begin{itemize}
    \item \textbf{Categorical Variables:} Missing values within the categorical features were imputed using the most frequent value observed for each respective feature. Subsequently, these categorical features were transformed into a numerical format using one-hot encoding, resulting in one-hot vectors for each category. This transformation was performed in Python utilizing the \texttt{OneHotEncoder} from the scikit-learn library.
    \item \textbf{Numeric Variables:} For numeric features, missing values were replaced with the median value of the corresponding variable. Following imputation, these numeric features were standardized to have a mean of 0 and a variance of 1. This normalization was achieved using the \texttt{StandardScaler} from scikit-learn.
\end{itemize}

Upon completion of these preprocessing steps—imputing missing values and converting all features to a numeric representation—the dataset expanded to a dataframe with 105 features. This increased dimensionality makes the visualization even harder. To address this challenge, we apply t-SNE dimensionality reduction technique on the processed data, which will be discussed in \underline{Section xxx}.

Moreover, as for the dataset itself, we may see that the number of samples not evenly fall into the two classes, as one class has \underline{xxxxxx} while the other one has \underline{xxxxxxxx}. This may cause troubles when training models, which will be discussed in \underline{Section xxx}.

\input{assets/tab_tsne_performance}

\subsection{Data visualization}

\input{assets/fig_tsne}

The t-SNE dimensionality reduction technique, known for its strength in preserving local similarities between data samples, was employed for initial data exploration through visualization. For better preserve the information within the original data, we are using different hyperparameter settings to perform the dimensionality reduction, and the results are illustrated in \underline{Table xxx}, through which we may see that the perplexity of \underline{value xxxx} can strike a balance between local and global properties, bringing us best visualization effects. And since the 2D and 3D may bring us different information, in the future we may use the two dimension setting at the same time.

With the proper value of perplexity, we visualize the dataset in \underline{Figure xxx}. This visual analysis revealed several key characteristics of the dataset. Firstly, it highlighted the potential presence of multiple sub-clusters, even within the same ground truth class label. This observation suggests considerable diversity within classes, potentially indicating the existence of distinct sub-classes. Such granularity might necessitate fitting different predictive models to these identified sub-clusters should overall model performance prove unsatisfactory with a single global model.

Secondly, and significantly, the t-SNE visualization indicated that samples from the two primary income classes ( $\le50K$ and $>50K$) are not clearly distinguishable, often co-occurring within the same visual clusters despite their different labels. Specifically, samples with income less than 50K appear broadly distributed across the visualization, whereas samples with income greater than 50K tend to concentrate on one side of the plot. This pattern suggests that the $\le50K$ class is more general and heterogeneous, encompassing a wider range of data distributions, compared to the more localized $>50K$ class.

The observed overlap between classes in the t-SNE embedding, while potentially influenced to some extent by information loss inherent in any dimensionality reduction process, strongly implies that many features in the original high-dimensional dataset may not be effective at separating the two income classes. These less discriminative features could be causing samples from different classes to appear close to each other in the original feature space. Consequently, there might be a need to identify and prioritize more important, distinguishing features to enhance model performance, which is confirmed in our later experiments in \underline{Section xxx}. The current visualization also raises a concern that models trained directly on such an embedding, or even on the original features if they indeed lack strong separability, might yield suboptimal classification performance. Should this be the case, further investigation into preserving more dimensions (balancing computational efficiency with information retention) or more targeted feature engineering/selection would be warranted.


\subsection{Clustering analysis}

\input{assets/tab_clustering_performance}

\input{assets/fig_clustering}

\subsubsection*{Methods}

Observations from the t-SNE visualization suggested inherent clustering characteristics within the dataset. To further investigate these structures and gain deeper insights, a clustering analysis was performed using two distinct algorithms: K-means and agglomerative hierarchical clustering.

\paragraph{K-means} K-means is a widely recognized partitional clustering algorithm that aims to partition N observations into K clusters. The algorithm operates iteratively through two main steps:

\subparagraph{Assignment step} Each data point xi from the dataset X is assigned to the cluster whose centroid $\mu_k$ is closest. This assignment is determined by the formula: $r_i = \underset{k\in \{1\ldots K\} }{\arg\min}{ \|x_{i}-\mu_k\| }$.

\subparagraph{Update step} After all points are assigned to clusters, the centroid of each cluster is recalculated as the mean of all points assigned to it: $\mu_k = \frac{\sum_{i:r_i=k}^{} x_i}{\sum_{i:r_i=k}^{} 1}$. The quality of the initial centroid selection significantly impacts K-means performance. Therefore, the \textbf{k-means++} initialization technique was employed. This method selects the first centroid randomly from the data points. Subsequent centroids are chosen from the remaining data points with a probability proportional to their squared distance from the nearest existing centroid, repeating until $K$ centers are selected. This approach helps mitigate sensitivity to poor initializations and theoretically guarantees an $O(\log K)$-competitive solution. The K-means algorithm was implemented in Python using the \texttt{KMeans} class from the scikit-learn library.

\paragraph{Agglomerative clustering} Agglomerative hierarchical clustering employs a bottom-up strategy, initially treating each data point as an individual cluster. It then iteratively merges the closest pairs of clusters until only a single cluster containing all data points remains, or a pre-specified number of clusters is reached. The method for measuring the distance between clusters is critical to its performance, and here we are using Ward's method, which aims to minimize the total within-cluster sum of squares, and the cost of merging two clusters, $A$ and $B$, can be expressed as: $\Delta(A, B)=\frac{2n_An_b}{n_A+n_B}\|\mu_A-\mu_B\|^2$ where $n_A, n_B$ represent the number of elements in cluster $A$ and $B$, respectively. This algorithm was implemented in Python using the \texttt{AgglomerativeClustering} in scikit-learn.

The selection of these two methods was motivated by their distinct characteristics. K-means, being a classic and computationally efficient algorithm, provides a rapid baseline for clustering performance. Agglomerative hierarchical clustering was chosen due to observations from the t-SNE analysis (as seen in Figure \underline{xxxxx}), which suggested the presence of potential sub-clusters. It was hypothesized that a hierarchical approach might better capture the original data distribution and provide more nuanced insights into these finer-grained structures.

\subsubsection*{Results}

The outcomes of the K-means and agglomerative clustering algorithms were visualized in \underline{figure xxx} and quantitatively evaluated using several established metrics. To assess the intrinsic quality of the clusters, including their compactness and separation, the Silhouette Score, Calinski-Harabasz Score, and Davies-Bouldin Score were employed. Furthermore, the Adjusted Rand Score (ARI) was utilized as an external metric to evaluate the extent to which the resulting clusters align with the ground truth labels of the dataset.

The analysis of these metrics revealed an interesting divergence in performance. Regarding the internal quality metrics, K-means demonstrated superior performance compared to agglomerative clustering. The higher Silhouette Score and Calinski-Harabasz Score, alongside a lower Davies-Bouldin Score for K-means, indicate that it formed clusters with comparatively better intra-cluster similarity and greater inter-cluster separation from a geometric perspective.

However, when evaluated using the external Adjusted Rand Index, agglomerative hierarchical clustering outperformed K-means. This finding suggests that while K-means may produce geometrically "neater" clusters (often spherical, due to its objective function), these do not align as closely with the actual underlying class distributions as the clusters produced by the hierarchical method. This discrepancy implies that the true data distribution is complex, and an over-reliance on preferred geometric shapes (a characteristic of K-means) might yield high scores on shape-focused internal metrics but fail to capture the nuanced information present in the ground truth labels. Moreover, the not balanced distribution of the two labels may also be one of the reasons behind the bad performance of K-means, which assumes that the clusters have similar densities and sizes.

The observed diversity within one of the ground truth classes, particularly the tendency for these samples to exhibit sub-clustering (as hinted at in the t-SNE visualization, e.g., the right-hand side of Figure \underline{xxxx}), leads to the hypothesis that these sub-structures might be influenced by noisy or less discriminative features that prevent clear separation, making the clustering performance not satisfactory. Agglomerative clustering will be further explored in future analysis to determine if it can more effectively delineate these potential sub-clusters.

\subsection{Prediction: training and testing}

\subsubsection*{Methods}

Based on the t-SNE visualization and clustering analysis, we may see that the data distribution is quite complicated and it might not be easy to find a simple, clear line or a plane in a hyperspace which can correct set the samples apart. Therefore, we think that simple logistic models may not be good enough to perform the classification task.  Therefore, as for prediction, we would like to use decision tree based models for better performance, since it can create a more complex decision space and boundary, and as for the logistic regression, we use this method in comparison of tree based methods.

\paragraph{Logistic regression} Logistic regressions 123456.

\paragraph{Decision trees} Decision trees 123456.

\subsubsection{Configuration}

As for the logistic regression model, we are using \underline{here describe the hyperparameters for the model}\ldots

Since the logistic regression model is deterministic, \underline{(describe the training configuration and the training process)}\ldots.

As for the gradient boosting model XGBoost, \underline{(describe the training configuration and the training process)}

\subsubsection*{Results}

\input{assets/fig_vanilla_models}

\underline{visualization of decision boundaries with different ways}

After fitting the model with the train data, we then get the trained model and the corresponding decision boundary is also visualized in the \underline{Figure}. However, since the model is fitted in high dimensional space, it is hard to visualize the decision boundary in 2D or 3D space. To do so, at first we just tried to iterate through all the data points in the t-SNE space and classify and colour them based on the prediction of our model, and the decision boundary we obtained is in \underline{Figure xxx}. However, t-SNE dimensionality reduction is an non-linear transformation which may twist the original decision boundary and we cannot get any useful information from it. Instead, we then tried to use these two methods:

\begin{itemize}
    \item PCA: We try to use PCA to visualize the decision boundary as it is a linear transformation. Although it cannot provide us details about whether the decision boundary is complex or not, it can tell us which principle component is most important in telling the classes apart. We firstly perform PCA analysis on the original dataset, then get the transformation matrices, which is later used to transform the decision boundary. The decision boundary is visualized in \underline{Figure xxxx} and we can see, that both logistic regression and decision model tells the samples apart based on information in the feature \underline{A, B, C}. 
    \item \underline{search for more methods to visualize the decision boundaries}
 %   \item Directly reducing data: \underline{After fitting the decision tree model / performing the PCA analysis}, we can see the importance of the feature \underline{A, B, C} and therefore, visualizing the decision boundary in the 3D space constructed with \underline{A, B, C } can somehow reflect the complexity of our model's decision boundaries while not make any 扭曲 of the original decison boundary. Then the decision boundary is visualized in \underline{Figure xxx}. From this 
\end{itemize}

From the figure we can see that the decision tree model is more expressive, as it can bring us multiple decision areas and decision boundaries while the logistic regression model cannot handle the complex data distribution and its decision boundary and the decision boundary just go cross some of the samples. 
This intuition implied by the visualization is also supported by the metrics. From \underline{Figure xxxx} we can see that for decision tree model, it can achieve better performance in terms of accuracy and f1 scores. 

However, it is quite clear that the performance is still not amazingly great, as the accuracy is just around \underline{1234} which is far below the myth of tree based models. Since we think the power of tree models is from its ability to create flexible decision areas, we think that adding the depth of the trees or increasing the number of trees in the process of fitting may be a good idea. We then increase the depth of a single decision tree to \underline{1245} and did more experiments.

\input{assets/fig_depth_dt}

while we used the ensemble models to do more experiments. Their decision boundary are visualized in \underline{figure xxx} and the performance metrics are in \underline{table xxx}. Based on the results above, we may see that the ability of classification does increase as the tree goes deeper, 
\underline{(but as the tree goes too deep, the performance on the test set will decrease as the model start to overfit)}. 
And for the decision boundaries, we can see that the two models can bring us more complex decision boundaries, making the prediction more precise. 

\input{assets/fig_ensemble_models}

Also, we may see the importance of features for further support on our conclusion and the results can be referred to in \underline{fig xxx}. For the best performance model XGBoost, we can see that the importance of features is significantly imbalanced as the \underline{xxxxx} matters most while the model pays little attention on the other features. For the vanilla decision tree model and the random forest model, they pay attention to more features but result in worse performance. This interesting discovery align with the mechanism of XGBoost and the rest, as the gradient boosting model will try to learn from its failures and may be more able to learn the features which can make it away from error, while the random forest or the vanilla decision models will pay equal attention on all of the features and will be more likely affected by noises. This fact also aligns to our t-SNE visualization in \underline{figure} as it clearly shows the existence of noisy features and
only a few of features really matters.

Meanwhile, the performance on one class is significantly worse than another class, which may due to the not balanced distribution in the train data.

\subsubsection*{Model selection}

\input{assets/fig_model_selection}

In this task, our goal is to find a model class which can utilize the given data as much as possible while it can still generalize the learnt knowledge on unseen data. To evaluate the model performance, we are using the ROC curve and the AUC value illustrated in \underline{Table xxxx}, from which we can see that the XGBoost model with depth \underline{xxxxx} and \underline{(other parameters)} has the highest AUC value. Hence, in future tasks, we may use XGBoost to perform classification tasks on this dataset. 

\subsection{Prediction: Training and Testing}

\subsubsection*{Methods}

Insights derived from the t-SNE visualization and preceding clustering analysis suggest that the dataset exhibits a complex underlying distribution. This complexity implies that a simple linear separator or a hyperplane in the high-dimensional feature space may not be sufficient to accurately distinguish between the classes. Consequently, it is hypothesized that simpler models, such as standard logistic regression, might yield suboptimal performance on this classification task. Therefore, for the prediction phase, decision tree-based models were primarily investigated due to their inherent capability to construct more intricate decision spaces and boundaries. Logistic regression was included as a baseline for comparison against these tree-based methods.

\paragraph{Logistic Regression} \underline{Logistic regressions 123456.}

\paragraph{Decision Trees} \underline{Decision trees 123456.}

\subsubsection*{Configuration}

For the logistic regression model, the following hyperparameter settings were utilized: 

\underline{here describe the hyperparameters for the model}\ldots

Given that the logistic regression model is deterministic under fixed hyperparameters, the training process involved 

\underline{(describe the training configuration and the training process)}\ldots.

For the gradient boosting model, XGBoost, the training configuration was set as follows: 

\underline{(describe the training configuration and the training process)}.

\subsubsection*{Results}

\input{assets/fig_vanilla_models}

\underline{visualization of decision boundaries with different ways} % This seems like a note for yourself, so I'll keep it commented or you can integrate its meaning below.

Following model training on the designated training data, the resultant decision boundaries were visualized, as depicted in \underline{Figure [Reference to specific figure, e.g., Figure 3 or 4]}. Visualizing decision boundaries derived from models fitted in a high-dimensional space presents a challenge when projecting to 2D or 3D. An initial attempt involved iterating through data points in the t-SNE embedded space, classifying them with our trained high-dimensional model, and color-coding accordingly to infer a boundary (shown in \underline{Figure xxx}). However, due to the non-linear nature of t-SNE, which can significantly distort the original decision boundary geometry, this approach did not yield readily interpretable information.

Subsequently, alternative methods for decision boundary visualization were explored:

\begin{itemize}
\item PCA-based Visualization: Principal Component Analysis (PCA) was employed to visualize the decision boundary, leveraging its property as a linear transformation. While PCA may not fully capture the complexity of a non-linear boundary, it can reveal which principal components are most influential in class separation. PCA was first performed on the original dataset to obtain the transformation matrices. These matrices were then used to project an approximation of the decision boundary. The resulting visualization (see \underline{Figure xxxx}) suggests that both logistic regression and decision tree models primarily utilize information from features \underline{A, B, C} for class discrimination.
\item \underline{search for more methods to visualize the decision boundaries}
%    \item Directly reducing data: \underline{After fitting the decision tree model / performing the PCA analysis}, we can see the importance of the feature \underline{A, B, C} and therefore, visualizing the decision boundary in the 3D space constructed with \underline{A, B, C } can somehow reflect the complexity of our model's decision boundaries while not make any 扭曲 (distortion) of the original decison boundary. Then the decision boundary is visualized in \underline{Figure xxx}. From this
\end{itemize}

The visualizations indicate that the decision tree model possesses greater expressiveness, capable of defining multiple, distinct decision areas and boundaries. In contrast, the logistic regression model appears less adept at handling the complex data distribution, with its decision boundary often transecting groups of samples from different classes. This visual intuition is corroborated by quantitative performance metrics. As shown in \underline{Figure xxxx [Reference to performance metrics figure/table]}, the decision tree model achieved superior performance in terms of accuracy and F1-scores compared to logistic regression.

Despite this, however, the overall performance of the initial decision tree model, with an accuracy around \underline{1234}, was not exceptional and fell short of the typical efficacy associated with tree-based models on complex datasets. Recognizing that the strength of tree models lies in their capacity to create flexible decision regions, we hypothesized that increasing tree depth or employing ensemble methods could yield improvements. Accordingly, the depth of a single decision tree was increased to \underline{1245}, and further experiments were conducted. The results are shown in \underline{Figure xxx}, which indeed confirm our arguments as the decision boundaries get more complicated while the accuracy increases. Note that in the end, the decision boundary is overally complicated to fit the train data, which causes the problem of overfitting and the model performance no longer increases as the depth of decision trees increases.

\input{assets/fig_depth_dt}

Concurrently, ensemble models (Random Forest and XGBoost) were also evaluated. The decision boundaries for these models are visualized in \underline{figure xxx}, and their performance metrics are detailed in \underline{table xxx}. The results also show that the ensemble models demonstrate the capability to generate more complex and robust decision boundaries, contributing to more precise predictions. 

\underline{based on the data and performance, we can discuss more on the depth and overfitting of ensemble models}

\input{assets/fig_ensemble_models}

Feature importance analysis, presented in \underline{fig xxx}, provides additional support for our conclusions. For the top-performing model, XGBoost, feature importance is notably imbalanced: feature \underline{xxxxx} exhibits the highest importance, while other features receive considerably less attention. In contrast, the standard decision tree and Random Forest models distribute importance more broadly across features, which correlates with their comparatively weaker performance. This observation aligns with the operational mechanisms of these algorithms; gradient boosting models like XGBoost iteratively learn from errors and are adept at identifying features that most effectively contribute to error reduction, whereas standard decision trees and Random Forest models may give more uniform consideration to all features, making them potentially more susceptible to noise. This finding is also consistent with our earlier t-SNE visualizations (\underline{figure [Reference to t-SNE figure]}), which suggested the presence of noisy or less informative features and highlighted that only a few features might be critical for class separation.

It was also observed that model performance on one class was significantly poorer than on the other, potentially attributable to an imbalanced distribution of class labels in the training data.

\subsubsection*{Model Selection}

\input{assets/fig_model_selection}

The primary objective in this task is to identify a model class that effectively utilizes the available data while maintaining strong generalization capabilities on unseen data. Model performance was evaluated using the Receiver Operating Characteristic (ROC) curve and the corresponding Area Under the Curve (AUC) value, as illustrated in \underline{Table xxxx [or Figure xxxx if it's a plot]}. From these results, the XGBoost model, configured with a depth of \underline{xxxxx} and \underline{(other parameters)}, achieved the highest AUC value. Consequently, XGBoost is recommended for future classification tasks on this dataset.

\section{Conclusion}

This project undertook an exploration of the Adult Income dataset, focusing on its intrinsic characteristics and the performance of various classification models. After filling the missing values and performing one-hot encoding, our analysis revealed that the dataset contains a significant number of redundant features. These features offer limited utility to the classification task but contribute to increased computational costs and can negatively impact model performance, underscoring that only a subset of features is truly influential.

Initial exploratory data analysis, including visualization with t-SNE and clustering, supported the hypothesis that the dataset possesses a complex underlying distribution, which in turn suggests that the optimal decision boundary for classification is likely to be intricate.

In the subsequent classification phase, decision tree models were selected for their ability to capture complex decision boundaries, with logistic regression serving as a contrasting simpler model. While the initial performance of these models met basic expectations, there was clear scope for improvement. Consequently, more sophisticated approaches were investigated, including increasing the depth of individual decision trees and employing ensemble models like Random Forest and XGBoost. The results indicated that deeper decision trees can achieve better classification accuracy by forming more detailed decision boundaries, although excessively deep trees risk overfitting to the training data.

Among the ensemble models, both Random Forest and XGBoost demonstrated strong fitting capabilities. XGBoost, in particular, distinguished itself through its mechanism of learning from prior errors and its proficiency in identifying the most salient features for classification. Based on its highest AUC value, XGBoost demonstrated a superior balance between recall and precision, establishing it as a highly suitable model for this classification task.

Future work should focus on further enhancing classification performance. Potential avenues include more refined feature engineering, strategies to address class imbalance within the dataset, and conducting more extensive experiments with varied random seeds to robustly validate the current findings regarding decision tree depth and decision boundary complexity.

\section*{References}
{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}

\section*{Credit}
\begin{itemize}
    \item \textbf{Leyi WU}: 
    \item \textbf{Hua XU}: Draft all the report.
    \item \textbf{Jianhao RUAN}:
    \item \textbf{AI}: Formatting latex tables and figures, polishing the report (while all every single argument is made by us; this can be referred to with GitHub commit history)

\end{itemize}




\end{document}
