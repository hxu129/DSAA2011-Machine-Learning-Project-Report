\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025


% ready for submission
\usepackage[preprint]{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multicol}
\usepackage{multirow}
\usepackage{graphicx} % 虽然现在是占位符，但将来替换图片时会用到
\usepackage{subcaption} % 用于创建子图环境
\usepackage{makecell}
\usepackage{tabularray}

\title{Project Report \large \\ DSAA2011 Machine Learning}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  % examples of more authors
   Hua XU \\
   HKUST(GZ) \\
   \texttt{hxu401@connect.hkust-gz.edu.cn} \\
   \And
   Jianhao RUAN \\
   HKUST(GZ) \\
   \texttt{jruan189@connect.hkust-gz.edu.cn} \\
   \And
   Leyi WU \\
   HKUST(GZ) \\
   \texttt{lwu398@connect.hkust-gz.edu.cn} \\
}


\begin{document}


\maketitle

\section{Introduction}

\underline{don't forget to finish this part}


\section{Methods and results}

\underline{add an introductory sentence here}

\subsection{Data preprocessing}

\begin{itemize}
    \item \underline{observed patterns of dataset or insights}
\end{itemize}

The Adult Income dataset, employed in this project, is structured for binary classification and comprises 14 usable variables. These variables include six of integer type and eight of categorical type. Specific preprocessing strategies were applied to handle these different data types:

\begin{itemize}
    \item \textbf{Categorical Variables:} Missing values within the categorical features were imputed using the most frequent value observed for each respective feature. Subsequently, these categorical features were transformed into a numerical format using one-hot encoding, resulting in one-hot vectors for each category. This transformation was performed in Python utilizing the \texttt{OneHotEncoder} from the scikit-learn library.
    \item \textbf{Numeric Variables:} For numeric features, missing values were replaced with the median value of the corresponding variable. Following imputation, these numeric features were standardized to have a mean of 0 and a variance of 1. This normalization was achieved using the \texttt{StandardScaler} from scikit-learn.
\end{itemize}

Upon completion of these preprocessing steps—imputing missing values and converting all features to a numeric representation—the dataset expanded to a dataframe with 105 features. This increased dimensionality makes the visualization even harder. To address this challenge, we apply t-SNE dimensionality reduction technique on the processed data, which will be discussed in \underline{Section xxx}.

Moreover, as for the dataset itself, we may see that the number of samples not evenly fall into the two classes, as one class has \underline{xxxxxx} while the other one has \underline{xxxxxxxx}. This may cause troubles when training models, which will be discussed in \underline{Section xxx}.

and facilitate effective visualization, t-SNE was employed as a dimensionality reduction technique. Both 2D and 3D embeddings of the data were generated. Based on an initial assessment [referencing your Table 1 which shows KL divergence], the 3D t-SNE embedding was selected for subsequent analyses due to its superior ability to preserve information compared to the 2D embedding.


\input{assets/tab_tsne_performance}



\subsection{Data visualization}

\input{assets/fig_tsne}

The t-SNE dimensionality reduction technique, known for its strength in preserving local similarities between data samples, was employed for initial data exploration through visualization. For better preserve the information within the original data, we are using different hyperparameter settings to perform the dimensionality reduction, and the results are illustrated in \underline{Table xxx}, through which we may see that the perplexity of \underline{value xxxx} can strike a balance between local and global properties, bringing us best visualization effects. And since the 2D and 3D may bring us different information, in the future we may use the two dimension setting at the same time.

With the proper value of perplexity, we visualize the dataset in \underline{Figure xxx}. This visual analysis revealed several key characteristics of the dataset. Firstly, it highlighted the potential presence of multiple sub-clusters, even within the same ground truth class label. This observation suggests considerable diversity within classes, potentially indicating the existence of distinct sub-classes. Such granularity might necessitate fitting different predictive models to these identified sub-clusters should overall model performance prove unsatisfactory with a single global model.

Secondly, and significantly, the t-SNE visualization indicated that samples from the two primary income classes ( $\le50K$ and $>50K$) are not clearly distinguishable, often co-occurring within the same visual clusters despite their different labels. Specifically, samples with income less than 50K appear broadly distributed across the visualization, whereas samples with income greater than 50K tend to concentrate on one side of the plot. This pattern suggests that the $\le50K$ class is more general and heterogeneous, encompassing a wider range of data distributions, compared to the more localized $>50K$ class.

The observed overlap between classes in the t-SNE embedding, while potentially influenced to some extent by information loss inherent in any dimensionality reduction process, strongly implies that many features in the original high-dimensional dataset may not be effective at separating the two income classes. These less discriminative features could be causing samples from different classes to appear close to each other in the original feature space. Consequently, there might be a need to identify and prioritize more important, distinguishing features to enhance model performance, which is confirmed in our later experiments in \underline{Section xxx}. The current visualization also raises a concern that models trained directly on such an embedding, or even on the original features if they indeed lack strong separability, might yield suboptimal classification performance. Should this be the case, further investigation into preserving more dimensions (balancing computational efficiency with information retention) or more targeted feature engineering/selection would be warranted.


\subsection{Clustering analysis}

\input{assets/tab_clustering_performance}

\input{assets/fig_clustering}

\subsubsection*{Methods}

Observations from the t-SNE visualization suggested inherent clustering characteristics within the dataset. To further investigate these structures and gain deeper insights, a clustering analysis was performed using two distinct algorithms: K-means and agglomerative hierarchical clustering.

\paragraph{K-means} K-means is a widely recognized partitional clustering algorithm that aims to partition N observations into K clusters. The algorithm operates iteratively through two main steps:

\subparagraph{Assignment step} Each data point xi from the dataset X is assigned to the cluster whose centroid $\mu_k$ is closest. This assignment is determined by the formula: $r_i = \underset{k\in \{1\ldots K\} }{\arg\min}{ \|x_{i}-\mu_k\| }$.

\subparagraph{Update step} After all points are assigned to clusters, the centroid of each cluster is recalculated as the mean of all points assigned to it: $\mu_k = \frac{\sum_{i:r_i=k}^{} x_i}{\sum_{i:r_i=k}^{} 1}$. The quality of the initial centroid selection significantly impacts K-means performance. Therefore, the \textbf{k-means++} initialization technique was employed. This method selects the first centroid randomly from the data points. Subsequent centroids are chosen from the remaining data points with a probability proportional to their squared distance from the nearest existing centroid, repeating until $K$ centers are selected. This approach helps mitigate sensitivity to poor initializations and theoretically guarantees an $O(\log K)$-competitive solution. The K-means algorithm was implemented in Python using the \texttt{KMeans} class from the scikit-learn library.

\paragraph{Agglomerative clustering} Agglomerative hierarchical clustering employs a bottom-up strategy, initially treating each data point as an individual cluster. It then iteratively merges the closest pairs of clusters until only a single cluster containing all data points remains, or a pre-specified number of clusters is reached. The method for measuring the distance between clusters is critical to its performance, and here we are using Ward's method, which aims to minimize the total within-cluster sum of squares, and the cost of merging two clusters, $A$ and $B$, can be expressed as: $\Delta(A, B)=\frac{2n_An_b}{n_A+n_B}\|\mu_A-\mu_B\|^2$ where $n_A, n_B$ represent the number of elements in cluster $A$ and $B$, respectively. This algorithm was implemented in Python using the \texttt{AgglomerativeClustering} in scikit-learn.

The selection of these two methods was motivated by their distinct characteristics. K-means, being a classic and computationally efficient algorithm, provides a rapid baseline for clustering performance. Agglomerative hierarchical clustering was chosen due to observations from the t-SNE analysis (as seen in Figure \underline{xxxxx}), which suggested the presence of potential sub-clusters. It was hypothesized that a hierarchical approach might better capture the original data distribution and provide more nuanced insights into these finer-grained structures.

\subsubsection*{Results}

The outcomes of the K-means and agglomerative clustering algorithms were visualized in \underline{figure xxx} and quantitatively evaluated using several established metrics. To assess the intrinsic quality of the clusters, including their compactness and separation, the Silhouette Score, Calinski-Harabasz Score, and Davies-Bouldin Score were employed. Furthermore, the Adjusted Rand Score (ARI) was utilized as an external metric to evaluate the extent to which the resulting clusters align with the ground truth labels of the dataset.

The analysis of these metrics revealed an interesting divergence in performance. Regarding the internal quality metrics, K-means demonstrated superior performance compared to agglomerative clustering. The higher Silhouette Score and Calinski-Harabasz Score, alongside a lower Davies-Bouldin Score for K-means, indicate that it formed clusters with comparatively better intra-cluster similarity and greater inter-cluster separation from a geometric perspective.

However, when evaluated using the external Adjusted Rand Index, agglomerative hierarchical clustering outperformed K-means. This finding suggests that while K-means may produce geometrically "neater" clusters (often spherical, due to its objective function), these do not align as closely with the actual underlying class distributions as the clusters produced by the hierarchical method. This discrepancy implies that the true data distribution is complex, and an over-reliance on preferred geometric shapes (a characteristic of K-means) might yield high scores on shape-focused internal metrics but fail to capture the nuanced information present in the ground truth labels. Moreover, the not balanced distribution of the two labels may also be one of the reasons behind the bad performance of K-means, which assumes that the clusters have similar densities and sizes.

The observed diversity within one of the ground truth classes, particularly the tendency for these samples to exhibit sub-clustering (as hinted at in the t-SNE visualization, e.g., the right-hand side of Figure \underline{xxxx}), leads to the hypothesis that these sub-structures might be influenced by noisy or less discriminative features that prevent clear separation, making the clustering performance not satisfactory. Agglomerative clustering will be further explored in future analysis to determine if it can more effectively delineate these potential sub-clusters.

\subsection{Prediction: training and testing}

\underline{need to describe the chosen classification target, model classes and why they were selected}

\underline{need to describe the training process}


\subsubsection*{Methods}

Based on the t-SNE visualization and clustering analysis, we may see that the data distribution is quite complicated and it might not be easy to find a simple, clear line or a plane in a hyperspace which can correct set the samples apart. Therefore, we think that simple logistic models may not be good enough to perform the classification task.  Therefore, as for prediction, we would like to use decision tree based models for better performance, since it can create a more complex decision space and boundary, and as for the logistic regression, we use this method in comparison of tree based methods.

\paragraph{Logistic regression} Logistic regressions 123456.

\paragraph{Decision trees} Decision trees 123456.

\subsubsection*{Results}

\input{assets/fig_vanilla_models}

\underline{visualization of decision boundaries with different ways}

After fitting the model with the train data, we then get the trained model and the corresponding decision boundary is also visualized in the \underline{Figure}. However, since the model is fitted in high dimensional space, it is hard to visualize the decision boundary in 2D or 3D space. To do so, at first we just tried to iterate through all the data points in the t-SNE space and classify and colour them based on the prediction of our model, and the decision boundary we obtained is in \underline{Figure xxx}. However, t-SNE dimensionality reduction is an non-linear transformation which may twist the original decision boundary and we cannot get any useful information from it. Instead, we then tried to use these two methods:

\begin{itemize}
    \item PCA: We try to use PCA to visualize the decision boundary as it is a linear transformation. Although it cannot provide us details about whether the decision boundary is complex or not, it can tell us which principle component is most important in telling the classes apart. We firstly perform PCA analysis on the original dataset, then get the transformation matrices, which is later used to transform the decision boundary. The decision boundary is visualized in \underline{Figure xxxx} and we can see, that both logistic regression and decision model tells the samples apart based on information in the feature \underline{A, B, C}. 
    \item \underline{search for more methods to visualize the decision boundaried}
    \item %Directly reducing data: \underline{After fitting the decision tree model / performing the PCA analysis}, we can see the importance of the feature \underline{A, B, C} and therefore, visualizing the decision boundary in the 3D space constructed with \underline{A, B, C } can somehow reflect the complexity of our model's decision boundaries while not make any 扭曲 of the original decison boundary. Then the decision boundary is visualized in \underline{Figure xxx}. From this 
\end{itemize}

From the figure we can see that the decision tree model is more expressive, as it can bring us multiple decision areas and decision boundaries while the logistic regression model cannot handle the complex data distribution and its decision boundary and the decision boundary just go cross some of the samples. 

This intuition implied by the visualization is also supported by the metrics. From \underline{Figure xxxx} we can see that for decision tree model, it can achieve better performance in terms of accuracy and f1 scores. 

However, it is quite clear that the performance is still not amazingly great, as the accuracy is just around \underline{1234} which is far below the legend of tree based models. Since we think the power of tree models is from its ability to create flexible decision areas, we think that adding the depth of the trees or increasing the number of trees in the process of fitting may be a good idea. We then increase the depth of a single decision tree to \underline{1245} and did more experiments,
while we used the ensemble models to do more experiments. Their decision boundary are visualized in \underline{figure xxx} and the performance metrics are in \underline{table xxx}. Based on the results above, we may see that the ability of classification does increase as the tree goes deeper, 
\underline{(but as the tree goes too deep, the performance on the test set will decrease as the model start to overfit)}. 
And for the decision boundaries, we can see that the two models can bring us more complex decision boundaries, making the prediction more precise. 

\input{assets/fig_ensemble_models}

Also, we may see the importance of features for further support on our conclusion and the results can be referred to in \underline{fig xxx}. For the best performance model XGBoost, we can see that the importance of features is significantly imbalanced as the \underline{xxxxx} matters most while the model pays little attention on the other features. For the vanilla decision tree model and the random forest model, they pay attention to more features but result in worse performance. This
interesting discovery align with the mechanism of XGBoost and the rest, as the gradient boosting model will try to learn from its failures and may be more able to learn the features which can make it away from error, while the random forest or the vanilla decision models will pay equal attention on all of the features and will be more likely affected by noises. This fact also aligns to our t-SNE visualization in \underline{figure} as it clearly shows the existence of noisy features and
only a few of features really matters.

Meanwhile, the performance on one class is significantly worse than another class, which may due to the not balanced distribution in the train data.

\underline{check whether we have already explicitly compare the model performance}

\subsubsection*{Model selection}

\input{assets/fig_model_selection}

In this task, our goal for selecting models is to find a 

After fitting the models, we can select the model to use by analyzing their performance. The classification results are illustrated in \underline{table xxxxxxx} and based on that, we can calculate the ROC and AUC in \underline{fig xxx}. It shows that model \underline{xxxx} has the greatest AUC and it can balance well on the data


\section{Conclusion}

In this project, we try to explore the intrinsic characteristics of the Adult Income dataset and try to perform classification tasks with different models on it. Through our analysis, we find that the dataset contains a lot of redundant features which do not help the task too much but still will be involved in the classification process, making the costs big and the performance bad. Only a few features matter. With this observation, we then visualized the dataset with clustering methods, and through our analysis of the clustering methods, we have confirmed our hypothesis that the distribution of the data might be complex, so as the decision boundary. For better performance, we use decision tree since it can create complex decision boundary and we use logistic regression in contrast. While the performance of the models does meet our expectation, it is not yet good enough. Then we start to create more complex decision boundaries with deeper decision trees and more complicated ensemble models. The results shows that as the trees get deeper, its decision boundary will also get deeper, making the classification good while too deep trees may overfit on the training set. As for the ensemble models, both of them can fit well on the given data, while XGBoost can learn from its errors and learn the most important features. Since it has the lowest \underline{AUC} value, it can balance well between recall and precision, making it suitable for performing classification on this dataset. In the future, more experiments need to be done to achieve better classification performance, including doing better feature engineering, dealing with imbalanced data. More experiments with different random seeds are also needed to validate the discoveries about the depth of decision trees, the complexity of decision boundaries and so on.

\underline{we may need to find one or two papers to support our conclusion that only the specific feature is important on the dataset}

\section*{References}
{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}

\section*{Credit}
\begin{itemize}
    \item \textbf{Leyi WU}: 
    \item \textbf{Hua XU}: Draft all the report.
    \item \textbf{Jianhao RUAN}:
    \item \textbf{AI}: Formatting latex tables and figures, polishing the report (while all every single argument is made by us; this can be referred to with GitHub commit history)

\end{itemize}




\end{document}
