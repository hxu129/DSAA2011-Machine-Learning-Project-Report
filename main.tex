\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025


% ready for submission
\usepackage[preprint]{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multicol}
\usepackage{multirow}
\usepackage{graphicx} % 虽然现在是占位符，但将来替换图片时会用到
\usepackage{subcaption} % 用于创建子图环境
\usepackage{makecell}
\usepackage{tabularray}

\title{Project Report \large \\ DSAA2011 Machine Learning}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  % examples of more authors
   Hua XU \\
   HKUST(GZ) \\
   \texttt{hxu401@connect.hkust-gz.edu.cn} \\
   \And
   Jianhao RUAN \\
   HKUST(GZ) \\
   \texttt{jruan189@connect.hkust-gz.edu.cn} \\
   \And
   Leyi WU \\
   HKUST(GZ) \\
   \texttt{lwu398@connect.hkust-gz.edu.cn} \\
}


\begin{document}


\maketitle

\section{Introduction}

\underline{don't forget to finish this part}


\section{Methods and results}

\underline{add an introductory sentence here}

\subsection{Data preprocessing}

\begin{itemize}
    \item \underline{observed patterns of dataset or insights}
\end{itemize}

The Adult Income datasets is a binary classification dataset with 14 usable variables, six of which are integer types while the rest eight variables are categorical. In this project, we are using different strategy to process the data.

\begin{itemize}
    \item As for the categorical variables, we firstly use the most frequent value to impute the missing values in the data, then we use one-hot encoding to convert the values into one-hot vectors. This step is implemented in Python with \texttt{OneHotEncoder} in scikit-learn.
    \item As for the numeric variables, we use median values of the variables to replace the missing values, and after dealing with the missing values, we then normalize the data to $0$ mean and $1$ variance with \texttt{StandardScaler} in scikit-learn.
\end{itemize}

After imputing missing values and converting all features to numeric ones, we then get a wide dataframe with 105 features, making the computation costs high and taking much time in training and testing. For efficiency and visualization purposes, we then use t-SNE to reduce the dimensionality of the original data and get both the 2D and 3D embedding for future usages. Since then we would like to use 3D embedding for future usage since it can preserve more information based on the table.

\input{assets/tab_tsne_performance}

\underline{justify why we should use t-sne in 3D: comparing the time and performance}


\subsection{Data visualization}

\input{assets/fig_tsne}

The t-SNE dimensionality reduction technique, known for its strength in preserving local similarities between data samples, was employed for initial data exploration through visualization. This visual analysis revealed several key characteristics of the dataset. Firstly, it highlighted the potential presence of multiple sub-clusters, even within the same ground truth class label. This observation suggests considerable diversity within classes, potentially indicating the existence of distinct sub-classes. Such granularity might necessitate fitting different predictive models to these identified sub-clusters should overall model performance prove unsatisfactory with a single global model.

Secondly, and significantly, the t-SNE visualization indicated that samples from the two primary income classes ( $\le50K$ and $>50K$) are not clearly distinguishable, often co-occurring within the same visual clusters despite their different labels. Specifically, samples with income less than 50K appear broadly distributed across the visualization, whereas samples with income greater than 50K tend to concentrate on one side of the plot. This pattern suggests that the $\le50K$ class is more general and heterogeneous, encompassing a wider range of data distributions, compared to the more localized $>50K$ class.

The observed overlap between classes in the t-SNE embedding, while potentially influenced to some extent by information loss inherent in any dimensionality reduction process, strongly implies that many features in the original high-dimensional dataset may not be effective at separating the two income classes. These less discriminative features could be causing samples from different classes to appear close to each other in the original feature space. Consequently, there might be a need to identify and prioritize more important, distinguishing features to enhance model performance, which is confirmed in our later experiments in \underline{Section xxx}. The current visualization also raises a concern that models trained directly on such an embedding, or even on the original features if they indeed lack strong separability, might yield suboptimal classification performance. Should this be the case, further investigation into preserving more dimensions (balancing computational efficiency with information retention) or more targeted feature engineering/selection would be warranted.


\subsection{Clustering analysis}

\input{assets/tab_clustering_performance}

\input{assets/fig_clustering}

\subsubsection*{Methods}

Observations from the t-SNE visualization suggested inherent clustering characteristics within the dataset. To further investigate these structures and gain deeper insights, a clustering analysis was performed using two distinct algorithms: K-means and agglomerative hierarchical clustering.

\paragraph{K-means} K-means is a widely recognized partitional clustering algorithm that aims to partition N observations into K clusters. The algorithm operates iteratively through two main steps:

\begin{enumerate}
    \item \textbf{Assignment step:} Each data point xi from the dataset X is assigned to the cluster whose centroid $\mu_k$ is closest. This assignment is determined by the formula: $r_i = \underset{k\in \{1\ldots K\} }{\arg\min}{ \|x_{i}-\mu_k\| }$.
    \item \textbf{Update step:} After all points are assigned to clusters, the centroid of each cluster is recalculated as the mean of all points assigned to it: $\mu_k = \frac{\sum_{i:r_i=k}^{} x_i}{\sum_{i:r_i=k}^{} 1}$. The quality of the initial centroid selection significantly impacts K-means performance. Therefore, the \textbf{k-means++} initialization technique was employed. This method selects the first centroid randomly from the data points. Subsequent centroids are chosen from the remaining data points with a probability proportional to their squared distance from the nearest existing centroid, repeating until $K$ centers are selected. This approach helps mitigate sensitivity to poor initializations and theoretically guarantees an $O(\log K)$-competitive solution. The K-means algorithm was implemented in Python using the \texttt{KMeans} class from the scikit-learn library.
\end{enumerate}

\paragraph{Agglomerative clustering} Agglomerative hierarchical clustering employs a bottom-up strategy, initially treating each data point as an individual cluster. It then iteratively merges the closest pairs of clusters until only a single cluster containing all data points remains, or a pre-specified number of clusters is reached. The method for measuring the distance between clusters is critical to its performance, and here we are using Ward's method, which aims to minimize the total within-cluster sum of squares, and the cost of merging two clusters, $A$ and $B$, can be expressed as: $\Delta(A, B)=\frac{2n_An_b}{n_A+n_B}\|\mu_A-\mu_B\|^2$ where $n_A, n_B$ represent the number of elements in cluster $A$ and $B$, respectively. This algorithm was implemented in Python using the \texttt{AgglomerativeClustering} in scikit-learn.

The selection of these two methods was motivated by their distinct characteristics. K-means, being a classic and computationally efficient algorithm, provides a rapid baseline for clustering performance. Agglomerative hierarchical clustering was chosen due to observations from the t-SNE analysis (as seen in Figure \underline{xxxxx}), which suggested the presence of potential sub-clusters. It was hypothesized that a hierarchical approach might better capture the original data distribution and provide more nuanced insights into these finer-grained structures.

\subsubsection*{Results}

The outcomes of the K-means and agglomerative clustering algorithms were visualized in \underline{figure xxx} and quantitatively evaluated using several established metrics. To assess the intrinsic quality of the clusters, including their compactness and separation, the Silhouette Score, Calinski-Harabasz Score, and Davies-Bouldin Score were employed. Furthermore, the Adjusted Rand Score (ARI) was utilized as an external metric to evaluate the extent to which the resulting clusters align with the ground truth labels of the dataset.

The analysis of these metrics revealed an interesting divergence in performance. Regarding the internal quality metrics, K-means demonstrated superior performance compared to agglomerative clustering. The higher Silhouette Score and Calinski-Harabasz Score, alongside a lower Davies-Bouldin Score for K-means, indicate that it formed clusters with comparatively better intra-cluster similarity and greater inter-cluster separation from a geometric perspective.

However, when evaluated using the external Adjusted Rand Index, agglomerative hierarchical clustering outperformed K-means. This finding suggests that while K-means may produce geometrically "neater" clusters (often spherical, due to its objective function), these do not align as closely with the actual underlying class distributions as the clusters produced by the hierarchical method. This discrepancy implies that the true data distribution is complex, and an over-reliance on preferred geometric shapes (a characteristic of K-means) might yield high scores on shape-focused internal metrics but fail to capture the nuanced information present in the ground truth labels. Moreover, the not balanced distribution of the two labels may also be one of the reasons behind the bad performance of K-means, which assumes that the clusters have similar densities and sizes.

The observed diversity within one of the ground truth classes, particularly the tendency for these samples to exhibit sub-clustering (as hinted at in the t-SNE visualization, e.g., the right-hand side of Figure \underline{xxxx}), leads to the hypothesis that these sub-structures might be influenced by noisy or less discriminative features that prevent clear separation, making the clustering performance not satisfactory. Agglomerative clustering will be further explored in future analysis to determine if it can more effectively delineate these potential sub-clusters.

\subsection{Prediction: training and testing}

\underline{need to describe the chosen classification target, model classes and why they were selected}

\underline{need to describe the training process}


\subsubsection*{Methods}

Based on the t-SNE visualization we may see that it is very hard to find a continuous line or surface to separate the samples from the two different classes apart. 
Therefore, as for prediction, we would like to use decision tree based models and logistic regressions, as the decision tree based models may create discrete decision boundaries while logistic regression may bring us a simple decision boundary, making the classification precise.

\paragraph{Logistic regression} Logistic regressions 123456.

\paragraph{Decision trees} Decision trees 123456.

\subsubsection*{Results}

\input{assets/fig_vanilla_models}


After fitting the model with the train data, we then get the trained model and the corresponding decision boundary is also visualized in the \underline{figure}. From the figure we can see that the decision tree model is more expressive, as it can bring us multiple decision areas and decision boundaries while the logistic regression model cannot handle the complex data distribution and its decision boundary and the decision boundary just go cross some of the samples. 


This intuition implied by the visualization is also supported by the metrics. From the table \underline{below} we can see that as for the overall performance. 

\underline{interpretion of results}


However, it is quite clear that the performance is still not amazingly great, as the accuracy is just around \underline{1234} which is far below the legend of tree based models. Since we think the power of tree models is from its ability to create flexible decision areas, we think that adding the depth of the trees or increasing the number of trees in the process of fitting may be a good idea. We then increase the depth of a single decision tree to \underline{1245} and did more experiments,
while we used the ensemble models to do more experiments. Their decision boundary are visualized in \underline{figure xxx} and the performance metrics are in \underline{table xxx}. Based on the results above, we may see that the ability of classification does increase as the tree goes deeper, 
\underline{(but as the tree goes too deep, the performance on the test set will decrease as the model start to overfit)}. 
And for the decision boundaries, we can see that the two models can bring us more complex but robust decision boundaries.

\input{assets/fig_ensemble_models}

Also, we may see the importance of features for further support on our conclusion and the results can be referred to in \underline{fig xxx}. For the best performance model XGBoost, we can see that the importance of features is significantly imbalanced as the \underline{xxxxx} matters most while the model pays little attention on the other features. For the vanilla decision tree model and the random forest model, they pay attention to more features but result in worse performance. This
interesting discovery align with the mechanism of XGBoost and the rest, as the gradient boosting model will try to learn from its failures and may be more able to learn the features which can make it away from error, while the random forest or the vanilla decision models will pay equal attention on all of the features and will be more likely affected by noises. This fact also aligns to our t-SNE visualization in \underline{figure} as it clearly shows the existence of noisy features and
only a few of features really matters.

Meanwhile, the performance on one class is significantly worse than another class, which may due to the not balanced distribution in the train data.

\underline{check whether we have already explicitly compare the model performance}

\subsubsection*{Model selection}

\input{assets/fig_model_selection}

After fitting the models, we can select the model to use by analyzing their performance. The classification results are illustrated in \underline{table xxxxxxx} and based on that, we can calculate the ROC and AUC in \underline{fig xxx}. It shows that model \underline{xxxx} has the greatest AUC and it can balance well on the data


\section{Conclusion}

In this project, we try to explore the intrinsic characteristics of the Adult Income dataset and try to perform classification tasks with different models on it. Through our analysis, we find that the dataset contains a lot of redundant features which do not help the task too much but still will be involved in the classification process, making the costs big and the performance bad. Only a few features matter. For better time efficiency, we firstly perform dimensionality reduction
with t-SNE and found that the classes are hard to separate and there are some sub-clusters in the dataset. 

\underline{then we visualized the dataset with clustering methods}

Then we start to perform classification task on it. For the complex data distribution, logistic regression perform badly on the dataset as it cannot create flexible decision boundary. Tree based models performs good on the dataset, as the performance will increase as the trees get deeper. Ensemble models is the king on this task, among those the XGBoost is the good good winner, as which can learn from its failures and find the most important feature. However, the model still suffer from the
imbalanced distribution of the two labels and in the future, more advanced techniques should be applied for better performance.

\underline{we may need to find one or two papers to support our conclusion that only the specific feature is important on the dataset}





\label{conclusion}

\section*{References}
{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}

\section*{Credit}
\begin{itemize}
    \item \textbf{Leyi WU}: 
    \item \textbf{Hua XU}: 
    \item \textbf{Jianhao RUAN}:
    \item \textbf{AI}: Formatting latex tables and figures, polishing the report (while all every single argument is made by us; this can be referred to with GitHub commit history)

\end{itemize}




\end{document}
