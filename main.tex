\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025


% ready for submission
\usepackage[preprint]{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multicol}
\usepackage{multirow}
\usepackage{graphicx} % 虽然现在是占位符，但将来替换图片时会用到
\usepackage{subcaption} % 用于创建子图环境
\usepackage{makecell}
\usepackage{tabularray}

\title{Project Report \large \\ DSAA2011 Machine Learning}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  % examples of more authors
   Hua XU \\
   HKUST(GZ) \\
   \texttt{hxu401@connect.hkust-gz.edu.cn} \\
   \And
   Jianhao RUAN \\
   HKUST(GZ) \\
   \texttt{jruan189@connect.hkust-gz.edu.cn} \\
   \And
   Leyi WU \\
   HKUST(GZ) \\
   \texttt{lwu398@connect.hkust-gz.edu.cn} \\
}


\begin{document}


\maketitle

\section{Introduction}

The Adult Income dataset, a benchmark for binary classification, presents a common challenge in machine learning: predicting whether an individual's income exceeds \$50,000 per year based on a set of demographic and socio-economic features. This project undertakes a comprehensive analysis of this dataset, beginning with data preprocessing and exploratory visualization, followed by clustering analysis to uncover intrinsic data structures, and culminating in the development and evaluation of various classification models.

Initial data exploration, including the application of t-SNE for dimensionality reduction and visualization, revealed the complex nature of the dataset. Key observations included the high dimensionality resulting from feature encoding, the apparent difficulty in separating the two income classes in a low-dimensional embedding, and the potential existence of sub-clusters within classes. Furthermore, visual inspection hinted at the presence of many redundant or less informative features, a hypothesis later corroborated by feature importance analysis from predictive models.

The core of this study involves a systematic comparison of different machine learning techniques. Clustering analysis using K-means and agglomerative hierarchical clustering was performed to understand the data's natural groupings. While K-means produced geometrically well-defined clusters according to internal metrics, agglomerative clustering showed better alignment with ground truth labels, suggesting the true class distributions are complex and not ideally suited to K-means' assumptions. Subsequently, several classification models were evaluated, ranging from logistic regression and basic decision trees to more advanced ensemble methods like Random Forest and XGBoost.

Our findings highlight that while simpler models provide baseline performance, ensemble methods, particularly XGBoost, achieve superior classification accuracy. XGBoost's ability to learn from errors and focus on the most pertinent features proved advantageous, aligning with the initial observation that only a few features are critical for this task. This report details the methodologies employed, presents the analytical results, and discusses the implications of these findings, including the impact of feature redundancy and class imbalance on model performance, ultimately identifying XGBoost as the most effective model for this dataset. Future work will explore strategies for feature engineering and addressing class imbalance to further enhance predictive accuracy.

\section{Methods and results}

This section details the sequential pipeline of our data analysis, commencing with data preprocessing, followed by exploratory data visualization, clustering analysis to uncover underlying groupings, and culminating in the training and evaluation of predictive classification models.

\subsection{Data preprocessing}

\begin{itemize}
    \item \underline{observed patterns of dataset or insights}
\end{itemize}

The Adult Income dataset, employed in this project, is structured for binary classification and comprises 14 usable variables. These variables include six of integer type and eight of categorical type. Specific preprocessing strategies were applied to handle these different data types:

\begin{itemize}
    \item \textbf{Categorical Variables:} Missing values within the categorical features were imputed using the most frequent value observed for each respective feature. Subsequently, these categorical features were transformed into a numerical format using one-hot encoding, resulting in one-hot vectors for each category. This transformation was performed in Python utilizing the \texttt{OneHotEncoder} from the scikit-learn library.
    \item \textbf{Numeric Variables:} For numeric features, missing values were replaced with the median value of the corresponding variable. Following imputation, these numeric features were standardized to have a mean of 0 and a variance of 1. This normalization was achieved using the \texttt{StandardScaler} from scikit-learn.
\end{itemize}

Upon completion of these preprocessing steps—imputing missing values and converting all features to a numeric representation—the dataset expanded to a dataframe with 105 features. This increased dimensionality makes the visualization even harder. To address this challenge, we apply t-SNE dimensionality reduction technique on the processed data, which will be discussed in \underline{Section xxx}.

Moreover, as for the dataset itself, we may see that the number of samples not evenly fall into the two classes, as one class has \underline{xxxxxx} while the other one has \underline{xxxxxxxx}. This may cause troubles when training models, which will be discussed in \underline{Section xxx}.

\input{assets/tab_tsne_performance}

\subsection{Data visualization}

\input{assets/fig_tsne}

The t-SNE dimensionality reduction technique, known for its strength in preserving local similarities between data samples, was employed for initial data exploration through visualization. For better preserve the information within the original data, we are using different hyperparameter settings to perform the dimensionality reduction, and the results are illustrated in \underline{Table xxx}, through which we may see that the perplexity of \underline{value xxxx} can strike a balance between local and global properties, bringing us best visualization effects. And since the 2D and 3D may bring us different information, in the future we may use the two dimension setting at the same time.

With the proper value of perplexity, we visualize the dataset in \underline{Figure xxx}. This visual analysis revealed several key characteristics of the dataset. Firstly, it highlighted the potential presence of multiple sub-clusters, even within the same ground truth class label. This observation suggests considerable diversity within classes, potentially indicating the existence of distinct sub-classes. Such granularity might necessitate fitting different predictive models to these identified sub-clusters should overall model performance prove unsatisfactory with a single global model.

Secondly, and significantly, the t-SNE visualization indicated that samples from the two primary income classes ( $\le50K$ and $>50K$) are not clearly distinguishable, often co-occurring within the same visual clusters despite their different labels. Specifically, samples with income less than 50K appear broadly distributed across the visualization, whereas samples with income greater than 50K tend to concentrate on one side of the plot. This pattern suggests that the $\le50K$ class is more general and heterogeneous, encompassing a wider range of data distributions, compared to the more localized $>50K$ class.

The observed overlap between classes in the t-SNE embedding, while potentially influenced to some extent by information loss inherent in any dimensionality reduction process, strongly implies that many features in the original high-dimensional dataset may not be effective at separating the two income classes. These less discriminative features could be causing samples from different classes to appear close to each other in the original feature space. Consequently, there might be a need to identify and prioritize more important, distinguishing features to enhance model performance, which is confirmed in our later experiments in \underline{Section xxx}. The current visualization also raises a concern that models trained directly on such an embedding, or even on the original features if they indeed lack strong separability, might yield suboptimal classification performance. Should this be the case, further investigation into preserving more dimensions (balancing computational efficiency with information retention) or more targeted feature engineering/selection would be warranted.


\subsection{Clustering analysis}

\input{assets/tab_clustering_performance}

\input{assets/fig_clustering}

\subsubsection*{Methods}

Observations from the t-SNE visualization suggested inherent clustering characteristics within the dataset. To further investigate these structures and gain deeper insights, a clustering analysis was performed using two distinct algorithms: K-means and agglomerative hierarchical clustering.

\paragraph{K-means} K-means is a widely recognized partitional clustering algorithm that aims to partition N observations into K clusters. The algorithm operates iteratively through two main steps:

\subparagraph{Assignment step} Each data point xi from the dataset X is assigned to the cluster whose centroid $\mu_k$ is closest. This assignment is determined by the formula: $r_i = \underset{k\in \{1\ldots K\} }{\arg\min}{ \|x_{i}-\mu_k\| }$.

\subparagraph{Update step} After all points are assigned to clusters, the centroid of each cluster is recalculated as the mean of all points assigned to it: $\mu_k = \frac{\sum_{i:r_i=k}^{} x_i}{\sum_{i:r_i=k}^{} 1}$. The quality of the initial centroid selection significantly impacts K-means performance. Therefore, the \textbf{k-means++} initialization technique was employed. This method selects the first centroid randomly from the data points. Subsequent centroids are chosen from the remaining data points with a probability proportional to their squared distance from the nearest existing centroid, repeating until $K$ centers are selected. This approach helps mitigate sensitivity to poor initializations and theoretically guarantees an $O(\log K)$-competitive solution. The K-means algorithm was implemented in Python using the \texttt{KMeans} class from the scikit-learn library.

\paragraph{Agglomerative clustering} Agglomerative hierarchical clustering employs a bottom-up strategy, initially treating each data point as an individual cluster. It then iteratively merges the closest pairs of clusters until only a single cluster containing all data points remains, or a pre-specified number of clusters is reached. The method for measuring the distance between clusters is critical to its performance, and here we are using Ward's method, which aims to minimize the total within-cluster sum of squares, and the cost of merging two clusters, $A$ and $B$, can be expressed as: $\Delta(A, B)=\frac{2n_An_b}{n_A+n_B}\|\mu_A-\mu_B\|^2$ where $n_A, n_B$ represent the number of elements in cluster $A$ and $B$, respectively. This algorithm was implemented in Python using the \texttt{AgglomerativeClustering} in scikit-learn.

The selection of these two methods was motivated by their distinct characteristics. K-means, being a classic and computationally efficient algorithm, provides a rapid baseline for clustering performance. Agglomerative hierarchical clustering was chosen due to observations from the t-SNE analysis (as seen in Figure \underline{xxxxx}), which suggested the presence of potential sub-clusters. It was hypothesized that a hierarchical approach might better capture the original data distribution and provide more nuanced insights into these finer-grained structures.

\subsubsection*{Results}

The outcomes of the K-means and agglomerative clustering algorithms were visualized in \underline{figure xxx} and quantitatively evaluated using several established metrics. To assess the intrinsic quality of the clusters, including their compactness and separation, the Silhouette Score, Calinski-Harabasz Score, and Davies-Bouldin Score were employed. Furthermore, the Adjusted Rand Score (ARI) was utilized as an external metric to evaluate the extent to which the resulting clusters align with the ground truth labels of the dataset.

The analysis of these metrics revealed an interesting divergence in performance. Regarding the internal quality metrics, K-means demonstrated superior performance compared to agglomerative clustering. The higher Silhouette Score and Calinski-Harabasz Score, alongside a lower Davies-Bouldin Score for K-means, indicate that it formed clusters with comparatively better intra-cluster similarity and greater inter-cluster separation from a geometric perspective.

However, when evaluated using the external Adjusted Rand Index, agglomerative hierarchical clustering outperformed K-means. This finding suggests that while K-means may produce geometrically "neater" clusters (often spherical, due to its objective function), these do not align as closely with the actual underlying class distributions as the clusters produced by the hierarchical method. This discrepancy implies that the true data distribution is complex, and an over-reliance on preferred geometric shapes (a characteristic of K-means) might yield high scores on shape-focused internal metrics but fail to capture the nuanced information present in the ground truth labels. Moreover, the not balanced distribution of the two labels may also be one of the reasons behind the bad performance of K-means, which assumes that the clusters have similar densities and sizes.

The observed diversity within one of the ground truth classes, particularly the tendency for these samples to exhibit sub-clustering (as hinted at in the t-SNE visualization, e.g., the right-hand side of Figure \underline{xxxx}), leads to the hypothesis that these sub-structures might be influenced by noisy or less discriminative features that prevent clear separation, making the clustering performance not satisfactory. Agglomerative clustering will be further explored in future analysis to determine if it can more effectively delineate these potential sub-clusters.

\subsection{Prediction: Training and Testing}

\subsubsection*{Methods}

Insights derived from the t-SNE visualization and preceding clustering analysis suggest that the dataset exhibits a complex underlying distribution. This complexity implies that a simple linear separator or a hyperplane in the high-dimensional feature space may not be sufficient to accurately distinguish between the classes. Consequently, it is hypothesized that simpler models, such as standard logistic regression, might yield suboptimal performance on this classification task. Therefore, for the prediction phase, decision tree-based models were primarily investigated due to their inherent capability to construct more intricate decision spaces and boundaries. Logistic regression was included as a baseline for comparison against these tree-based methods.

\paragraph{Logistic Regression} \underline{Logistic regressions 123456.}

\paragraph{Decision Trees} \underline{Decision trees 123456.}

\paragraph{Logistic Regression} Logistic Regression is a statistical model used for binary classification tasks, and can be extended for multi-class problems. It models the probability of a binary outcome using a logistic function (sigmoid function) applied to a linear combination of input features. For a given input sample $x_t$ and parameters $\theta$ and $\theta_0$ (offset), the probability of observing the class label $y_t \in \{-1, +1\}$ can be expressed as:
$$ Pr(y_{t}|x_{t},\theta,\theta_{0}) = \frac{1}{1+\exp(-y_{t}(\langle\theta,x_{t}\rangle+\theta_{0}))} $$
The model parameters $(\theta, \theta_0)$ are typically learned by minimizing a loss function, often the negative log-likelihood, which for $n$ samples can be written as $\sum_{t=1}^{n}\log[1+\exp(-y_{t}(\langle x_{t},\theta\rangle+\theta_{0}))]$. To prevent overfitting and improve generalization, regularization techniques such as $L_2$-norm regularization are commonly incorporated, leading to an objective function like:
$$ \min_{(\theta,\theta_{0})\in\mathbb{R}^{d}\times\mathbb{R}}\frac{\lambda}{2}\|\theta\|^{2}+\sum_{t=1}^{n}\log[1+\exp(-y_{t}(\langle x_{t},\theta\rangle+\theta_{0}))] $$
where $\lambda$ is the regularization parameter. This optimization is often solved using gradient-based methods. In this project, Logistic Regression is implemented using the \texttt{LogisticRegression} class from scikit-learn.

\paragraph{Decision Trees} A Decision Tree is a non-parametric supervised learning method used for classification and regression tasks. It functions by constructing a tree-like model of decisions based on the input features. The tree is composed of decision nodes, which represent tests on specific features (e.g., $feature_A > value_X$), branches representing the outcomes of these tests, and leaf nodes (or terminal nodes) which represent the class labels (for classification) or continuous values (for regression). The construction of a decision tree involves a recursive partitioning of the dataset. At each node, the algorithm selects a feature $h^*(x)$ and a split point that best separates the data according to a certain criterion, such as minimizing impurity (e.g., Gini impurity or entropy) or maximizing information gain, which aims to reduce uncertainty at each step. This process continues until a stopping condition is met, for instance, when a node becomes pure (all samples in the node belong to the same class), a predefined maximum depth of the tree is reached, or the number of samples in a node falls below a minimum threshold. The resulting model is a hierarchical structure that is relatively easy to interpret and visualize. Decision tree models are implemented in Python using the \texttt{DecisionTreeClassifier} class from scikit-learn.

\subsubsection*{Configuration}

For the logistic regression model, the following hyperparameter settings were utilized: 

\underline{here describe the hyperparameters for the model}\ldots

Given that the logistic regression model is deterministic under fixed hyperparameters, the training process involved 

\underline{(describe the training configuration and the training process)}\ldots.

For the gradient boosting model, XGBoost, the training configuration was set as follows: 

\underline{(describe the training configuration and the training process)}.

\subsubsection*{Results}

\input{assets/fig_vanilla_models}

\underline{visualization of decision boundaries with different ways} % This seems like a note for yourself, so I'll keep it commented or you can integrate its meaning below.

Following model training on the designated training data, the resultant decision boundaries were visualized, as depicted in \underline{Figure [Reference to specific figure, e.g., Figure 3 or 4]}. Visualizing decision boundaries derived from models fitted in a high-dimensional space presents a challenge when projecting to 2D or 3D. An initial attempt involved iterating through data points in the t-SNE embedded space, classifying them with our trained high-dimensional model, and color-coding accordingly to infer a boundary (shown in \underline{Figure xxx}). However, due to the non-linear nature of t-SNE, which can significantly distort the original decision boundary geometry, this approach did not yield readily interpretable information.

Subsequently, alternative methods for decision boundary visualization were explored:

\begin{itemize}
\item PCA-based Visualization: Principal Component Analysis (PCA) was employed to visualize the decision boundary, leveraging its property as a linear transformation. While PCA may not fully capture the complexity of a non-linear boundary, it can reveal which principal components are most influential in class separation. PCA was first performed on the original dataset to obtain the transformation matrices. These matrices were then used to project an approximation of the decision boundary. The resulting visualization (see \underline{Figure xxxx}) suggests that both logistic regression and decision tree models primarily utilize information from features \underline{A, B, C} for class discrimination.
\item \underline{search for more methods to visualize the decision boundaries}
%    \item Directly reducing data: \underline{After fitting the decision tree model / performing the PCA analysis}, we can see the importance of the feature \underline{A, B, C} and therefore, visualizing the decision boundary in the 3D space constructed with \underline{A, B, C } can somehow reflect the complexity of our model's decision boundaries while not make any 扭曲 (distortion) of the original decison boundary. Then the decision boundary is visualized in \underline{Figure xxx}. From this
\end{itemize}

The visualizations indicate that the decision tree model possesses greater expressiveness, capable of defining multiple, distinct decision areas and boundaries. In contrast, the logistic regression model appears less adept at handling the complex data distribution, with its decision boundary often transecting groups of samples from different classes. This visual intuition is corroborated by quantitative performance metrics. As shown in \underline{Figure xxxx [Reference to performance metrics figure/table]}, the decision tree model achieved superior performance in terms of accuracy and F1-scores compared to logistic regression.

Despite this, however, the overall performance of the initial decision tree model, with an accuracy around \underline{1234}, was not exceptional and fell short of the typical efficacy associated with tree-based models on complex datasets. Recognizing that the strength of tree models lies in their capacity to create flexible decision regions, we hypothesized that increasing tree depth or employing ensemble methods could yield improvements. Accordingly, the depth of a single decision tree was increased to \underline{1245}, and further experiments were conducted. The results are shown in \underline{Figure xxx}, which indeed confirm our arguments as the decision boundaries get more complicated while the accuracy increases. Note that in the end, the decision boundary is overally complicated to fit the train data, which causes the problem of overfitting and the model performance no longer increases as the depth of decision trees increases.

\input{assets/fig_depth_dt}

Concurrently, ensemble models (Random Forest and XGBoost) were also evaluated. The decision boundaries for these models are visualized in \underline{figure xxx}, and their performance metrics are detailed in \underline{table xxx}. The results also show that the ensemble models demonstrate the capability to generate more complex and robust decision boundaries, contributing to more precise predictions. 

\underline{based on the data and performance, we can discuss more on the depth and overfitting of ensemble models}

\input{assets/fig_ensemble_models}

Feature importance analysis, presented in \underline{fig xxx}, provides additional support for our conclusions. For the top-performing model, XGBoost, feature importance is notably imbalanced: feature \underline{xxxxx} exhibits the highest importance, while other features receive considerably less attention. In contrast, the standard decision tree and Random Forest models distribute importance more broadly across features, which correlates with their comparatively weaker performance. This observation aligns with the operational mechanisms of these algorithms; gradient boosting models like XGBoost iteratively learn from errors and are adept at identifying features that most effectively contribute to error reduction, whereas standard decision trees and Random Forest models may give more uniform consideration to all features, making them potentially more susceptible to noise. This finding is also consistent with our earlier t-SNE visualizations (\underline{figure [Reference to t-SNE figure]}), which suggested the presence of noisy or less informative features and highlighted that only a few features might be critical for class separation.

It was also observed that model performance on one class was significantly poorer than on the other, potentially attributable to an imbalanced distribution of class labels in the training data.

\subsubsection*{Model Selection}

\input{assets/fig_model_selection}

The primary objective in this task is to identify a model class that effectively utilizes the available data while maintaining strong generalization capabilities on unseen data. Model performance was evaluated using the Receiver Operating Characteristic (ROC) curve and the corresponding Area Under the Curve (AUC) value, as illustrated in \underline{Table xxxx [or Figure xxxx if it's a plot]}. From these results, the XGBoost model, configured with a depth of \underline{xxxxx} and \underline{(other parameters)}, achieved the highest AUC value. Consequently, XGBoost is recommended for future classification tasks on this dataset.

\section{Conclusion}

This project undertook an exploration of the Adult Income dataset, focusing on its intrinsic characteristics and the performance of various classification models. After filling the missing values and performing one-hot encoding, our analysis revealed that the dataset contains a significant number of redundant features. These features offer limited utility to the classification task but contribute to increased computational costs and can negatively impact model performance, underscoring that only a subset of features is truly influential.

Initial exploratory data analysis, including visualization with t-SNE and clustering, supported the hypothesis that the dataset possesses a complex underlying distribution, which in turn suggests that the optimal decision boundary for classification is likely to be intricate.

In the subsequent classification phase, decision tree models were selected for their ability to capture complex decision boundaries, with logistic regression serving as a contrasting simpler model. While the initial performance of these models met basic expectations, there was clear scope for improvement. Consequently, more sophisticated approaches were investigated, including increasing the depth of individual decision trees and employing ensemble models like Random Forest and XGBoost. The results indicated that deeper decision trees can achieve better classification accuracy by forming more detailed decision boundaries, although excessively deep trees risk overfitting to the training data.

Among the ensemble models, both Random Forest and XGBoost demonstrated strong fitting capabilities. XGBoost, in particular, distinguished itself through its mechanism of learning from prior errors and its proficiency in identifying the most salient features for classification. Based on its highest AUC value, XGBoost demonstrated a superior balance between recall and precision, establishing it as a highly suitable model for this classification task.

Future work should focus on further enhancing classification performance. Potential avenues include more refined feature engineering, strategies to address class imbalance within the dataset, and conducting more extensive experiments with varied random seeds to robustly validate the current findings regarding decision tree depth and decision boundary complexity.

\section*{References}
{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}

\section*{Credit}
\begin{itemize}
    \item \textbf{Hua XU}: Draft all the report.
    \item \textbf{Jianhao RUAN}:
    \item \textbf{Leyi WU}: 
    \item \textbf{AI}: Formatting latex tables and figures, polishing the report (while all every single argument is made by us; this can be referred to with GitHub commit history), drafted the Introduction section

\end{itemize}




\end{document}
